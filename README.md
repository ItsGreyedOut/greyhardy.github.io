# itsgreyedout.github.io
# 6/2021 - 12/2021
# Portfoilo of work produced while taking the Georgia Tech Data Science and Analytics Boot Camp.
# Georgia Tech Data Science and Analytics Boot Camp - Powered by Trilogy Education Services, a 2U, Inc. brand 1

Skills Gained:

Web Technologies and Data
Visualization
• HTML
• CSS
• Bootstrap
• Dashboarding
• JavaScript Charting
• Geomapping with Leaflet.js

Business Intelligence Software
• Tableau

Advanced Topics
• R Programming
• Big Data Analytics with Hadoop
• Supervised Machine Learning
• Unsupervised Machine Learning
• Deep Learning

Intermediate Excel
• Pivot Tables
• VBA Scripting

Fundamental Statistics
• Modelling
• Forecasting

Python Programming
• Python 3
• NumPy
• SciPy
• Pandas
• Matplotlib
• API Interactions

Databases
• PostgreSQL/pgAdmin
• MongoDB
• Extract-Transform-Load (ETL)

I'm qualified for many different roles, including:
Data Analyst
Data Engineer Database Administrator (Entry Level)
Data Scientist (Entry Level) Big Data Engineer (Entry Level)
Data Journalist Business Intelligence Analyst
Business Analyst Research Analyst
SQL Developer Software Engineer (Entry Level)
Computational Scientist
Data Architect

What I learned:
Utilize statistical analysis to characterize and interpret novel datasets.
Use advanced SQL and NoSQL techniques to combine multiple datasets into one so
as to create even more impressive and comprehensive databases.  Create basic
interactive websites and applications to show your work to the entire world.
Work with and lead small-scale teams in order to create applications and visual d
atasets.  Scrape information from web pages in order to collect data from a wide
variety of online sources.  Communicate and glean new business insights using
enterprise-grade tools like Tableau.  Build data-driven prediction algorithms
using machine learning tools and techniques.  Work independently or in a group
on complex data-mining projects.  Understand the basics of troubleshooting and
enhancing legacy code. Use version-controlling software such as Git to collaborate
on open-source software.  Employ statistical models to predict and forecast trends.
Build VBA scripts in Excel to automate tedious manual processes Utilize real-world 
data sources to showcase social, financial, and political phenomena.  Create Python-based
scripts to automate the cleanup, restructuring, and rendering of large, heterogeneous 
datasets.  Interact with RESTful APIs using Python.  Requests and JSON parsing techniques
Create in-depth graphs, charts, and tables utilizing a wide-variety of data-driven
programming languages and libraries.  Use ETL process (Extract, Transform, Load) to
transform and consolidate data from multiple sources.  Use geographic data to create
visually exciting, interactive, and informative maps.  Build custom interactive data 
isualizations using JavaScript libraries. Write SQL commands to perform Create, Read, 
Update, and Delete commands.  Over the course of 24 weeks, you’ll attend informative
lectures, participate in a variety of individual and team exercises, and work 
independently inside and outside of class time. Homework assignments provide an 
opportunity to apply what you’ve learned and build on it. The goal is to give you a 
comprehensive learning experience and true insight into a “day in the life” of a data 
professional.  

DISCUSSION PROJECT WORK PORTFOLIO PROJECTS
My portfolio signals to employers that I am ready for primetime! I built a
substantial portfolio of projects that demonstrate my abilities across a wide variety of
technologies.  I worked on timed in-class exercises and projects individually
and in teams to put classroom teachings into practice.  Instructor-led discussions covered
the background, history, and use of new technologies or concepts.


My Portfolio
It’s a fact: companies care about what you can do, not what you say you can do. The
curriculum taught meu how to put what you’ve learned to work. I covered real-world
data projects, ranging from visualizing bike sharing data in New York City to
mapping worldwide earthquakes in real-time.  


Bank Deserts Project
Social economists have long noted a trend that in geographic areas with higher poverty rates, there is often
a dearth of reputable banks or financial services. The shortage leads to higher rates of financial victimization
in these areas. But how could we show this trend using data? In this activity, I learned how to combine
data from the U.S. Census, Google Maps, and Google Places to visualize the relationship between various
socioeconomic factors and the number of banks in a given zip code.

Skills Needed:
• Python
• Pandas
• Google Maps
• Google Places
• Matplotlib
• APIs

Objectives
• Utilize the Python Requests library to make hundreds of API calls to the U.S. Census and Google Maps datasets
• Utilize the Python pandas library to organize the retrieved information by zip code and socioeconomicfactors
• Build scatter plots to easily communicate the Banking Desert phenomena
• Design statistical models to quantify relationships between factors

Earthquake History
Data isn’t just about finance and numbers. It can also be used for good as well. In this activity, you will create
an interactive visualization of historic earthquakes over time using Leaflet.js, a popular JavaScript geomapping
library. Your final application will provide a near-live feed of global earthquakes and their relative magnitudes.

Skills Needed
• HTML
• CSS
• Javascript
• Leaflet.js
• APIs
• JSON

Objectives
• Harness the power of APIs and JSON to gather earthquake data from USGS datasets
• Utilize Leaflet.js library to create visually compelling,animated maps
• Embed the created map onto a live web page using HTML and CSS

Non-Profit Investment Analysis
Using a dataset with more than 34,000 non-profit organizations that have received funding from Alphabet Soup
over the years, I developed a binary classifier neural network model to predict whether applicants will be
successful if funded by Alphabet Soup. I preprocessed data, design network structure, and train, evaluate,
and optimize a neural network model.

Skills Needed
• TensorFlow
• Keras
• Machine Learning
• Deep Learning

Objectives
• Use Python and Pandas to preprocess the data for training a deep neural network model
• Compile, train, and evaluate a deep neural network model with TensorFlow and Keras
• Communicate results of the model in comparison to other machine learning models

Web Scraping Application
Sometimes, data is just out of reach. Whether it’s a social media website that is guarding its information, a
government agency that has poorly organized records, or a cookbook website filled with secret recipes — data
isn’t always accessible by external applications. This is where data scraping comes in. Utilizing Python libraries
like Beautiful Soup, I learned to convert data straight from raw HTML into a queryable and storable form,
opening up troves of data for your future applications.

Skills Needed
• Python
• Beautiful Soup
• HTML
• CSS
• MongoDB

Objectives
• Scrape your favorite social media website for otherwise inaccessible data
• Parse through the retrieved information and store it into a MongoDB database
• Create new representations of the data using HTML and CSS

Game Studio Analytics
Congratulations! I have landed a job as the Lead Analyst for an independent game company and for my
first assignment I have been given the difficult task of analyzing data and creating a report for their latest
smash hit release. I will be using the Python Pandas Library and Jupyter Notebook to create demographic
and financial reports.

Skills Needed
• Python
• Jupyter Notebook
• Pandas Library

Objectives
• Use Python and the Pandas library to create a report containing a vast amount of data
• Make the data viewable using Jupyter Notebook • Find, analyze, and write up descriptions of observable
trends in the data

Classifying Yelp Reviews
A Nielsen report concluded that 82% of visitors to Yelp intended to make a purchase, so it’s no surprise that
companies take online customer reviews and ratings seriously. In this section of the course, I built an
application that can analyze reviews and tell you through Natural Language Processing whether it’s negative
or positive. This means you don’t have to have a human read every review that gets posted and respond
accordingly. You can instead have a machine flag negative reviews for you so you can trigger an action like
outreach and more.

Skills Needed
• PySpark
• Machine Learning
• Natural Language Processing

Objectives
• Perform Natural Language Processing with PySpark-ML
• Establish a big data processing pipeline to clean and process data
• Train and validate a Naive Bayes machine learning model that can make predictions from customer reviews

Curriculum By Module:
Module 1:
Excel Crash
Course
Learn to do more with Microsoft Excel. In this module we’ll
cover advanced topics like statistical modeling, forecasting
and prediction, pivot tables, and VBA scripting. You’ll even
learn to model historic stock trends – and hopefully, learn
to beat the market!
• Microsoft Excel
• VBA Script
• Statistics Modeling

Module 2:
Python Data
Analytics
Gain a strong foothold in one of today’s fundamental
programming languages. In the course of this module,
you’ll gain deep proficiencies with core Python, data
analytic tools like NumPy, Pandas, Matplotlib, and specific
libraries for interacting with web data like Requests and
BeautifulSoup.
• Python
• APIs
• JSON
• NumPy
• Pandas
• Matplotlib
• Beautiful Soup
• SciPly

Module 3:
Databases
Work with PostgreSQL and MongoDB to organize data into
well-structured and easily retrievable data formats.
• SQL
• PostgreSQL
• MongoDB
• ETL Process

Module 4:
Web Visualization
Building visualizations is of little benefit without a way
to communicate the message. In this module, you’ll be
learning the core technologies of web development (HTML,
CSS, and JavaScript) to create new, interactive data
visualizations that you can share with everyone on the web.
• HTML
• CSS
• JavaScript
• AJAX
• Leaflet

Module 5:
Advanced Topics
By program’s end, you’ll be immersed in new and indemand
topics like Tableau, Hadoop, and Machine
Learning.
• Tableau
• Hadoop
• Supervised Machine
Learning
• Unsupervised Machine
Learning
• Deep Learning

Module 6:
Final Project
Bring everything that you have learned in class altogether
to create an impressive data-visualization application with
a small team. Get creative and come up with something
cool to show off to the whole world!
• Dreaming up something
fantastic and understanding
the bounds of reasonable
and achievable
